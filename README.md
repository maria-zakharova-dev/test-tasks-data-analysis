# test-tasks-data-analysis
This repository aims to showcase analytical and programming skills when working with custom data sets


В архиве "data sets.zip" находятся 2 файла с данными. Анализ данных проводился с использованием средств языка программирования Python в интерактивной облачной среде разработки Google Colab. Файл с решением сохранен в формате .ipynb. Для просмотра содержимого файла следует использовать Google Colab или Jupyter Notebook. С условием заданий можно ознакомиться ниже.

## Задание 1.

Имеется файл "data_task1.txt" с временной статистикой работы асессоров (асессор – человек, выполняющий оценочную работу «оценщик») над однотипным заданием.

Формат файла: login, tid, Microtasks, assigned_ts, tclosed_ts.
Где login — логин асессора; tid — id оцениваемого задания (task id); Microtasks – количество микрозаданий в одном задании; assigned_ts — время резервирования системой задания для асессора; closed_ts — точное время завершения работы над заданием; разделитель — табуляция \t.

Задание может состоять из одного или несколько микрозаданий. Время резервирования задания (assigned_ts) указывает на тот момент, когда система назначила определенного асессора исполнителем этого задания. Этот момент может совпадать с временем начала работы асессора над заданием, а может и не совпадать (асессор может отойти выпить чаю, а потом приступить к заданию, асессор может выполнять предыдущее задание, в то время как за ним зарезервированы новые).

Предположим, что асессор за 30 секунд своего рабочего времени получает N рублей. Какую оплату вы считаете справедливой для выполнения асессором одного микрозадания из этого файла? Опишите подробно все этапы вашего решения.


## Задание 2.

Имеется файл "data_task2.csv" с различными оценками асессоров (асессор – человек, выполняющий оценочную работу «оценщик»).

Формат файла: login tuid docid jud cjud. Где login — логин асессора; uid — id асессора (user id); docid — id оцениваемого документа (document id); jud — оценка асессора (judgement); cjud — правильная оценка (correct judgement); разделитель — табуляция \t.

Оценки могут принимать значение [0, 1], т.е. задание, которое сделали асессоры, имеет бинарную шкалу.

Используя данные об оценках, установите, какие асессоры хуже всего справились с заданием. На какие показатели вы ориентировались и какие метрики вы использовали для ответа на этот вопрос? Можно ли предложить какие-то новые метрики для подсчета качества асессоров с учетом природы оценок у этого бинарного задания? Опишите подробно все этапы вашего решения.
